---
title: Software Testing Fundamentals
description: TBD
draft: true
sidebar:
  order: 2
---

Testing has always been a critical aspect of software development.

Even when you were first learning to program, before you knew much about formal software testing, you likely tested your code by inputting data and comparing the actual behavior against what you expected.

This comparison of actual versus expected behavior is the core of software testing.

Even **manual testing**, where a person (developer, tester, or end-user) runs code, manually enters input, and monitors the behavior, still plays a role in software development.

However, today, "testing" usually refers to **automated testing**, where a suite of tests is executed against a codebase. Each test automatically runs part or all of the code and compares the actual behavior against the expected behavior.

Automated testing is typically driven by developers, who design and implement the tests for the software they are building.

Therefore, it's important to understand the fundamentals of automated software testing: why we test, what types of tests we can perform, and how they work at a high level. We will explore these fundamentals here.

## Why Perform Software Testing?

The primary goal of automated software testing is to prevent software bugs from reaching production and affecting users.

The cost of software bugs can range from

- [minor annoyances](https://en.wikipedia.org/wiki/Minus_World) to 
- [major frustrations](https://www.theverge.com/2022/12/23/23524555/google-calendar-ios-android-app-spam-events) to 
- [significant discomfort](https://www.nytimes.com/2016/01/14/fashion/nest-thermostat-glitch-battery-dies-software-freeze.html) to 
- [costly disasters](https://en.wikipedia.org/wiki/Ariane_flight_V88) and even 
- [fatalities](https://en.wikipedia.org/wiki/Therac-25).

Depending on the report, inadequate software testing costs the US economy anywhere from [tens of billions](https://www.computerworld.com/article/2575560/study--buggy-software-costs-users--vendors-nearly--60b-annually.html) to over [two trillion dollars annually](https://www.it-cisq.org/the-cost-of-poor-quality-software-in-the-us-a-2022-report/), not to mention serious non-economic impacts like loss of life.

Beyond preventing damage, automated testing offers other compelling benefits, especially for developers.

Well-designed automated tests give us confidence that our code will work as intended. This confidence allows us to *modify* our code without excessive worry about breaking it. Good tests enable us to add features, refactor code, and rely on the tests to quickly catch mistakes before they reach users.

The process of implementing tests can also clarify what you want your software to *do*, leading to better-designed systems by forcing you to consider use cases, boundary conditions, and architecture decisions.

Investing time in writing good tests upfront can save time later by reducing the effort needed to maintain your software.

For these reasons, we want to become proficient software testers. As we learn, remember that while good tests increase our confidence, they don't guarantee bug-free code. As [Edsger Dijkstra said](https://en.wikiquote.org/wiki/Edsger_W._Dijkstra#1960s):

> “Testing shows the presence, not the absence of bugs.”

## What Makes a Good Test?

Let's examine the characteristics of a good automated software test.

Every good automated software test has these core features:

- It focuses on **a single, specific behavior** within a software project, such as calling a function, invoking an API method, or interacting with a specific UI element (like a form or button).
- It is based on **a specific input**, such as a set of function parameters, a value passed to an API, or a type of user interaction (like submitting a form).
- The behavior being tested has **observable results**, such as a function's return value, an API method's response body, or a change in the UI after a form submission.
- **We know the expected results** of the behavior for the specific input used in the test.
- The test occurs within **a controlled environment**, such as a single, isolated process on a dedicated machine.

Given these characteristics, a software test's primary job is **to compare the actual results against the expected results**. If they match, the test passes; otherwise, it fails.

Here are some additional considerations for implementing good software tests.

### Tests Should Be Unchanging

Good tests capture the correct behavior of the software. As long as that behavior remains consistent, the tests should not need to change. **Once a test is written, it should not need to be modified.**

Specifically, changes like refactoring, adding new features, or fixing bugs should not alter the existing behaviors of our software.

A good test helps ensure that our software's existing behavior remains consistent as we modify the code.

If we need to change existing tests when refactoring, adding features, or fixing bugs, it *may* indicate that our code changes are unintentionally impacting the software's existing behavior.

Poorly written tests may need to be changed. We'll examine the characteristics of poorly written tests below.

### Testing Implementation Details

When writing tests, we might be tempted to rely on internal implementation details.

For instance, consider a test that reads the value of a specific variable, calls a private function, or accesses parts of the code that are only intended for developers.

Imagine testing a feature where a dialog opens when a user clicks a button. The mechanism controlling this dialog might be a boolean variable named `showDialog`, which is set to true when the button is clicked.

It might seem logical to test this by verifying that `showDialog` is `true` after the button click. However, this approach relies on implementation details.

In general, relying on implementation details in tests should be avoided because it makes tests  or brittle. If the code is refactored—such as renaming or removing `showDialog`—the test would fail and require modification, even if the functionality remains unchanged.

The best practice is to write tests that interact with the software as a user would, using only public APIs and observing visible changes in the UI.

For example, instead of checking the value of `showDialog`, a better test would verify that the dialog is actually displayed in the UI after the button is clicked. This could involve checking for the presence of the dialog's text or other visible elements.

Throughout this course, we will explore tools and techniques for writing tests that simulate user interactions rather than relying on internal implementation details.

That said, there are scenarios where testing implementation details is necessary. This is particularly true when critical behavior cannot be observed through the public interface.

For example, you might need to test whether a data access function correctly retrieves data from a cache instead of a database. Since this behavior may not be evident from the function's return value, examining the function's internals might be required.

## The Developer User Vs. The End User

When we say you should "implement tests that interact with our software the way a user would", remember that there can be [two different kinds of users](https://kentcdodds.com/blog/avoid-the-test-user):

- **The End User**: a person who interacts directly with the code, such as by clicking buttons and typing text.
- **The Developer User**: another developer who uses your code in their own code, such as by incorporating a class you wrote into their application.

Regardless of which type of user your code has (or if it has both, like a reusable UI component), always implement tests using the interface that user would use. This means clicks and typing for end users, and public method calls for developer users. Don't test implementation details if you can avoid it.

## Determinism and Flaky Tests

One crucial characteristic of automated software tests is **determinism**. A good test should consistently produce the same results—either passing or failing—when run under the same conditions, such as identical code and inputs.

Tests that behave unpredictably, sometimes passing and sometimes failing without any changes to the code or test, are called **flaky tests**. These tests undermine developers' confidence in the test suite and can waste significant time as developers attempt to diagnose the cause of the failures.

To maintain reliable tests, it is essential to avoid implementing flaky tests whenever possible. Strategies for addressing this issue will be covered later in the course.

However, some degree of test flakiness may be unavoidable in certain scenarios. For instance, a test that depends on a network service might fail due to uncontrollable factors like network instability.

In such cases, the impact of flakiness can be mitigated by automatically rerunning any failing tests. This approach helps ensure that transient issues do not disrupt the overall testing process.

## Different Types Of Automated Software Tests

We'll focus on three main types of automated software tests: **unit tests**, **integration tests**, and **end-to-end tests**. These types are distinguished by their scope, or how much code is being tested. The difference between unit test and integration test is often a matter of degree, but the difference between integration test and end-to-end test is more pronounced.

Google classifies tests based on their size. *Small* tests run in a single process, *medium* tests run on a single machine, and *large* tests run wherever they want. This is a little different from the unit/integration/end-to-end classification, but it is a useful way to think about the size of tests.

A more common metric is the scope of the test, which refers to how much code is being validated (*not* executed). The three main types of tests are:

### Unit Tests

A **unit test** focuses on validating a small, specific piece of code, such as a single function or class. The term "unit test" originates from the idea of testing individual "units" of code.

For instance, a unit test might verify that a function returns the correct output for a given set of inputs or that a form's submit handler properly validates user input.

Unit tests are typically small and straightforward to implement. Developers often write multiple unit tests to evaluate a unit of code under various conditions, referred to as **test cases**.

For example, when testing a shipping address form, unit tests might check that the form:

- Accepts a valid address.
- Rejects invalid inputs (e.g., missing ZIP code or city name).
- Handles unusual inputs gracefully (e.g., a city name that is 257 characters long).

Tests for rare or edge-case inputs are known as **boundary cases** and are essential to include in a comprehensive test suite.

By covering a wide range of scenarios, unit tests provide confidence that the code behaves as expected in different situations.

However, the primary limitation of unit tests (as humorously illustrated by[many](https://natooktesting.wordpress.com/2017/08/24/x-unit-tests-0-integration-tests/) [clever gifs](https://media.tenor.com/7c9bvnQbGCIAAAAC/unittest-unit.gif)) is that they do not verify whether different units of code work together correctly. This is where integration tests come into play.

### Integration Tests

An **integration test** has a broader scope than a unit test and focuses on validating the *interactions* between multiple software components.

For instance, while a unit test might check that a shipping address form properly validates input, an integration test would verify that the form successfully passes a valid address to a shipping cost calculator, which then computes the correct shipping cost. This test specifically ensures the interaction between the form and the shipping cost calculator works as expected.

Another example of an integration test could involve verifying that a user's shipping address is correctly stored in the database after the form is submitted. This would validate the interaction between the address form and the database.

By testing how different parts of the code interact, integration tests provide greater confidence in the overall functionality of the system compared to unit tests.

However, integration tests are inherently more complex to implement and maintain due to their wider scope and the dependencies between components.

As a result, balancing the use of unit tests and integration tests is essential. We will explore strategies for achieving this balance later in the course.

### End-To-End (E2E) Tests

**End-to-end tests** (also known as functional tests) have the broadest scope, validating large, interconnected portions of the codebase as they work together. These tests often cover entire pages or user flows and may involve verifying the behavior of multiple application processes running on different machines (e.g., a web client, web/API server, and database server).

End-to-end tests evaluate an application's behavior through its user interface and are typically executed using a “robot” that simulates user actions, such as clicking buttons and typing input.

For example, an end-to-end test for an e-commerce application might validate the entire checkout process. The “robot” would simulate a user clicking the “checkout” button, entering shipping and payment information, clicking the “complete checkout” button, and receiving a transaction confirmation.

The implementation of an end-to-end test encodes the expected behavior of the application at each step. The testing “robot” then verifies that the actual behavior matches these expectations.

While end-to-end tests provide the highest level of confidence by simulating real user interactions with the entire application, they are also the most expensive to implement, run, and maintain.

Despite their cost, end-to-end tests are invaluable because they ensure the application behaves as intended for real users, offering the greatest assurance of reliability.

### Other Types Of Automated Software Tests

While this course focuses on unit, integration, and end-to-end tests, there are other types of automated software testing worth mentioning:

- **Performance Tests**: These tests ensure that a piece of software uses resources (e.g., runtime or memory) efficiently and stays within acceptable limits. For example, a performance test might measure how long a piece of code takes to execute. If the execution time exceeds a predefined threshold, the test fails.
- **Load Tests**: These tests evaluate whether an application can maintain its performance under heavy usage. Load tests often simulate a large number of users interacting with the application simultaneously. If the application slows down significantly or encounters errors during the test, it may indicate the need for code optimization or larger architectural changes, such as adding more servers.
- **Static Tests**: These tests analyze the code without executing it. They can catch syntax, typos, type errors, style violations, security risks, and other issues before the code is run. Static tests are often performed by a linter integrated into the code editor, which provides immediate feedback to developers as they write code, or by a static analyzer.

## Balancing Different Types Of Automated Tests

Most production software projects benefit from a combination of unit, integration, and end-to-end tests. However, determining the ideal proportion of each type to balance their advantages and disadvantages has been a topic of debate.

### The Testing Pyramid

One widely recognized model for balancing these tests is the [testing pyramid](https://martinfowler.com/bliki/TestPyramid.html):

<div class="flex items-center justify-center">
<div class="flex flex-col items-center w-full max-w-lg">
  <div class="bg-red-500 text-white text-center py-4 w-1/3 border-b-2 border-red-700 shadow-md">
      <h4 class="text-base font-semibold">E2E Tests</h4>
      <p class="text-sm">Few, fancy, user-obsessed, slowest, and pricey</p>
  </div>
  <div class="bg-yellow-400 text-gray-800 text-center py-6 w-2/3 border-b-2 border-yellow-600 shadow-md">
      <h4 class="text-base font-semibold">Integration Tests</h4>
      <p class="text-sm">Connecting the dots, moderately complex and expensive, slower</p>
  </div>
  <div class="bg-green-500 text-white text-center py-8 w-full border-b-2 border-green-700 shadow-md">
      <h4 class="text-base font-semibold">Unit Tests</h4>
      <p class="text-sm">Fast, focused, and cheap</p>
  </div>
  </div>
</div>

The testing pyramid emphasizes that unit tests are the fastest and least expensive to write, run, and maintain. Therefore, they should make up the majority of your test suite. As you move up the pyramid to integration and end-to-end tests, which are slower and more expensive, the number of these tests should decrease.

In summary, the pyramid suggests:

- Unit Tests: The majority of your tests, as they are fast and cost-effective.
- Integration Tests: A smaller proportion, focusing on interactions between components.
- End-to-End Tests: The fewest, as they are the slowest and most expensive.

### The Testing Trophy

Another model, the [testing trophy](https://kentcdodds.com/blog/static-vs-unit-vs-integration-vs-e2e-tests), offers a different perspective:

![](https://res.cloudinary.com/kentcdodds-com/image/upload/w_1100,q_auto,f_auto,b_rgb:e6e9ee/kentcdodds.com/content/blog/unit-vs-integration-vs-e2e-tests/banner)

The testing trophy suggests that most tests should be integration tests, with unit tests forming the next-largest proportion and end-to-end tests being the smallest. Additionally, the trophy highlights the importance of **static tests**, which analyze code without executing it.

Proponents of the testing trophy argue that it better reflects the confidence gained from each type of test. Confidence generally increases as you move up the pyramid or trophy, with end-to-end tests providing the highest assurance.

They also point out that advancements in testing tools have made integration tests faster and easier to maintain, making it more practical to rely on a larger suite of integration tests.

### Choosing the Right Balance

Both models provide valuable insights, but the ideal balance depends on your project's specific needs. The key is to leverage the strengths of each type of test while minimizing their weaknesses to create a reliable and efficient testing strategy.

## Test-Driven Development (TDD)

Test-Driven Development (TDD) is a software development strategy that prioritizes automated testing. It revolves around a repeating cycle of testing, coding, and refactoring, commonly known as the "red, green, refactor" cycle.

![](https://res.cloudinary.com/kentcdodds-com/image/upload/f_auto,q_auto,dpr_2.0,w_1600/v1625033508/kentcdodds.com/content/blog/when-i-follow-tdd/0.jpg)
*Courtesy of [Kent C. Dodds](https://kentcdodds.com/blog/when-i-follow-tdd)*

The TDD cycle consists of the following steps:

- ❌ **Test Fails:** Begin by writing a test that includes just enough code to check the behavior of the next feature you plan to implement. Since the feature hasn't been implemented yet, the test will fail, resulting in a "red" error message.
- ✅ **Test Passes:** Next, write the minimum amount of code necessary to make the test pass, resulting in a "green" success message. At this stage, focus on functionality rather than code design or elegance.
- 🔄 **Refactor:** Once the test passes, refactor your code to ensure it is clean and well-structured. As you refactor, the test should continue to pass. If the test fails, it indicates that the refactoring has introduced an issue.

This cycle is repeated for each new feature or behavior you want to add to your code.

A significant advantage of TDD is that writing the test first compels you to consider your code's interface and behavior upfront. The "red" phase inherently involves design considerations.

In essence, to create a test for code that doesn't yet exist, you must decide how you will call that code, what its behavior will be, and how it will communicate its results back to you.

Making these decisions within the context of a test forces you to approach your code from the perspective of its user (in this case, the test itself). This approach can lead to code with a clear and simple interface that is easy to use and understand.

A key to successful TDD implementation is working in small increments. Start by implementing a single test with a limited scope, and then write just enough code to make that test pass. Repeat this process until the entire feature is implemented. This often involves breaking down seemingly simple problems into even smaller, manageable steps.

It's important to note that TDD does *not* require implementing all tests upfront. In fact, doing so is considered an anti-pattern. Instead, TDD involves working on one test at a time.

To gain a better understanding of working in small increments with TDD, I highly recommend reading the "A TDD Example" section from the ["Test-Driven Development" chapter](https://www.jamesshore.com/v2/books/aoad2/test-driven_development) of The Art of Agile Development.

## Limits of Automated Testing and the Importance of Manual Testing

While automated software testing is a cornerstone of modern development, it's crucial to recognize its limitations. Automated testing isn't suitable for every task, and manual testing remains essential in certain situations.

Manual testing is particularly valuable when human creativity or judgment is required.

For instance, security testing often involves identifying complex and subtle vulnerabilities, a task where human exploratory testing excels compared to automated systems. Exploratory testing leverages human intuition and creativity to uncover vulnerabilities that automated tools might miss. Once a human identifies a security vulnerability, an automated test can then be created to ensure its permanent fix.

Similarly, assessing the quality of elements like search results, audio, or video is challenging to automate, as it relies on subjective human judgment. Systematic manual testing is typically the best approach for these evaluations.

## Additional Readings

- [Testing Overview](https://abseil.io/resources/swe-book/html/ch11.html) from Software Engineering at Google
- [Test Driven Development](https://martinfowler.com/bliki/TestDrivenDevelopment.html) by Martin Fowler
- [We Finally Agree On Unit Tests](https://www.youtube.com/watch?v=MbU-PKukdMw) by Theo (YouTube)
- [Different types of testing and the testing pyramid](https://learn.microsoft.com/en-us/training/modules/visual-studio-test-concepts/4-different-types-of-testing) from Microsoft

{/* you should rarely have to change tests when you refactor code */}
{/* mocking stuff in unit tests, less in integration tests */}